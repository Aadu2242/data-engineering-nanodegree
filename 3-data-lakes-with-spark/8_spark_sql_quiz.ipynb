{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Spark SQL Quiz\n",
    "\n",
    "This quiz uses the same dataset and most of the same questions from the earlier \"Quiz - Data Wrangling with Data Frames Jupyter Notebook.\" For this quiz, however, use Spark SQL instead of Spark Data Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data wrangling with Spark SQL\").getOrCreate()\n",
    "\n",
    "user_log = spark.read.json(\"data/sparkify_log_small.json\")\n",
    "\n",
    "user_log.createOrReplaceTempView(\"user_log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Which page did user id \"\"(empty string) NOT visit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            page|\n",
      "+----------------+\n",
      "|Submit Downgrade|\n",
      "|       Downgrade|\n",
      "|          Logout|\n",
      "|   Save Settings|\n",
      "|        Settings|\n",
      "|        NextSong|\n",
      "|         Upgrade|\n",
      "|           Error|\n",
      "|  Submit Upgrade|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT DISTINCT page \n",
    "    FROM user_log \n",
    "    WHERE page NOT IN (\n",
    "        SELECT DISTINCT page \n",
    "        FROM user_log \n",
    "        WHERE userId = ''\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Reflect\n",
    "\n",
    "Why might you prefer to use SQL over data frames? Why might you prefer data frames over SQL?\n",
    "\n",
    "## Response\n",
    "\n",
    "We should use SQL over data frames because of:\n",
    " - It is straight forward. You just specify in plain SQL what data you need.\n",
    " - SQL is a common language when talking about databases, it's more likely you don't have a much greater learning curve\n",
    " \n",
    "We should use data frames over SQL because of:\n",
    " - When you need to use more advanced features like adding a column to a dataframe, windowing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "How many female users do we have in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|  462|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT gender, COUNT(DISTINCT userId) AS count\n",
    "    FROM user_log \n",
    "    WHERE gender = 'F'\n",
    "    GROUP BY gender\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "How many songs were played from the most played artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|  artist|plays_count|\n",
      "+--------+-----------+\n",
      "|Coldplay|         83|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT artist, COUNT(ts) AS plays_count\n",
    "    FROM user_log\n",
    "    WHERE page = 'NextSong'\n",
    "    GROUP BY artist\n",
    "    ORDER BY plays_count DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (challenge)\n",
    "\n",
    "How many songs do users listen to on average between visiting our home page? Please round your answer to the closest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             mean|\n",
      "+-----------------+\n",
      "|6.898347107438017|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# create a numerical flag for wether the user is in the home page\n",
    "flag_homepage_visit = udf(lambda x: 1 if x == \"Home\" else 0, IntegerType())\n",
    "\n",
    "# after flagging all home visits, we use that new column to create a window between home visits\n",
    "windowval = Window.partitionBy(\"userId\").orderBy(desc(\"ts\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# add a new columns called phase with that window count\n",
    "user_log_valid = user_log.filter((user_log.page == 'NextSong') | (user_log.page == 'Home')) \\\n",
    "    .select('userId', 'page', 'ts') \\\n",
    "    .withColumn(\"visited_home\", flag_homepage_visit(col(\"page\"))) \\\n",
    "    .withColumn(\"phase\", Fsum(\"visited_home\").over(windowval))\n",
    "\n",
    "user_log_valid.createOrReplaceTempView(\"user_log\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT AVG(c) AS mean\n",
    "    FROM (\n",
    "        SELECT \n",
    "            userId, \n",
    "            COUNT(phase) AS c \n",
    "        FROM user_log \n",
    "        WHERE page = 'NextSong' \n",
    "        GROUP BY userId, phase\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
